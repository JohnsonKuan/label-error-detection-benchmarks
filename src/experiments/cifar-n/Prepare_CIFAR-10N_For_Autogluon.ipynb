{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27abf3a1-17fb-4c8b-9786-ac2bbf418696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"./cifar-10-100n/\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image as im\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.vision import ImagePredictor, ImageDataset\n",
    "import pickle\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import cleanlab\n",
    "\n",
    "from data.datasets import input_dataset\n",
    "from cross_validation_autogluon import cross_val_predict_autogluon_image_dataset\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e10642c-122b-4cdd-a254-6e7a9664cea1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mcross_val_predict_autogluon_image_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgluoncv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageClassificationDataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mout_folder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./cross_val_predict_run/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_splits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_params\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'holdout_frac'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mngpus_per_trial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtime_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Run stratified K-folds cross-validation with AutoGluon image model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "dataset : gluoncv.auto.data.dataset.ImageClassificationDataset\n",
       "  AutoGluon dataset for image classification.\n",
       "\n",
       "out_folder : str, default=\"./cross_val_predict_run/\"\n",
       "  Folder to save cross-validation results. Save results after each split (each K in K-fold).\n",
       "\n",
       "n_splits : int, default=3\n",
       "  Number of splits for stratified K-folds cross-validation.\n",
       "\n",
       "model_params : Dict, default={\"epochs\": 1, \"holdout_frac\": 0.2}\n",
       "  Passed into AutoGluon's `ImagePredictor().fit()` method.\n",
       "\n",
       "ngpus_per_trial : int, default=1\n",
       "  Passed into AutoGluon's `ImagePredictor().fit()` method.\n",
       "\n",
       "time_limit : int, default=7200\n",
       "  Passed into AutoGluon's `ImagePredictor().fit()` method.\n",
       "\n",
       "random_state : int, default=123\n",
       "  Passed into AutoGluon's `ImagePredictor().fit()` method.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "None\n",
       "\u001b[0;31mFile:\u001b[0m      /dcai/src/experiments/cross_validation_autogluon.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?cross_val_predict_autogluon_image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3efbf64-e7c4-4d42-8780-e4eea650355a",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fee790f-9a91-4b85-af55-ae91eb121541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels\n",
    "noise_file = torch.load('./cifar-10-100n/data/CIFAR-10_human.pt')\n",
    "clean_label = noise_file['clean_label']\n",
    "worst_label = noise_file['worse_label']\n",
    "aggre_label = noise_file['aggre_label']\n",
    "random_label1 = noise_file['random_label1']\n",
    "random_label2 = noise_file['random_label2']\n",
    "random_label3 = noise_file['random_label3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db8b30e-a276-435b-821b-f16ec987907c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/data/cifar-10-python.tar.gz\n",
      "Loaded worse_label from ./cifar-10-100n/data/CIFAR-10_human.pt.\n",
      "The overall noise rate is 0.40208\n",
      "noisy labels loaded from ./cifar-10-100n/data/CIFAR-10_human.pt\n",
      "The noisy data ratio in each class is [0.10618 0.12106 0.10684 0.09346 0.0808  0.11336 0.08406 0.10546 0.09258\n",
      " 0.0962 ]\n",
      "over all noise rate is  0.40208\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "dataset = \"cifar10\"\n",
    "noise_type = \"worse_label\"\n",
    "noise_path = \"./cifar-10-100n/data/CIFAR-10_human.pt\"\n",
    "is_human = True\n",
    "\n",
    "train_dataset, test_dataset, num_classes,num_training_samples = \\\n",
    "    input_dataset(dataset, noise_type, noise_path, is_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ddeae8-471b-460a-a36e-b66a2fcfdcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy Label: 3\n",
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F4794566700>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out some images\n",
    "idx = 40\n",
    "X = train_dataset.train_data[idx]\n",
    "noisy_label = train_dataset.train_noisy_labels[idx]\n",
    "label = train_dataset.train_labels[idx]\n",
    "print(f\"Noisy Label: {noisy_label}\")\n",
    "print(f\"Label: {label}\")\n",
    "im.fromarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9a669-ebcc-42b5-badc-0d450a866fe8",
   "metadata": {},
   "source": [
    "## Save images to folder\n",
    "\n",
    "AutoGluon image models require the data to be image files in a folder\n",
    "\n",
    "We'll save the images as PNG files in a single folder and save their file paths in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e8de96-fe3f-4176-ae85-09d204a46c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.5 s, sys: 8.24 s, total: 50.7 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create folder\n",
    "SAVE_PATH = \"./cifar-10n-png/train/\"\n",
    "os.makedirs(SAVE_PATH)\n",
    "\n",
    "# save images to folder\n",
    "\n",
    "image_file_paths = [] # list of image file paths\n",
    "\n",
    "for i, data in enumerate(train_dataset.train_data):\n",
    "    \n",
    "    # convert data to image object\n",
    "    image = im.fromarray(data) \n",
    "\n",
    "    # path to image PNG file\n",
    "    png_file = f\"{SAVE_PATH}image_id_{i}.png\"\n",
    "    \n",
    "    # print the number of images saved\n",
    "    print(f\"Images saved: {i + 1}\", end=\"\\r\")\n",
    "    \n",
    "    # append file to list of file paths\n",
    "    image_file_paths.append(png_file)\n",
    "    \n",
    "    # save image as PNG\n",
    "    image.save(png_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0d19f-3486-47e3-9cff-8847b47edc27",
   "metadata": {},
   "source": [
    "## Create DataFrame to use as input for AutoGluon Image predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f3fa582-97a0-4790-a93f-747731042804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AutoGluon requires the column names to be \"image\" and \"label\"\n",
    "df_train = pd.DataFrame({\n",
    "    \"image\": image_file_paths,\n",
    "    \"label\": train_dataset.train_noisy_labels # noisy labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "450e9485-900a-42aa-b749-0288aab1d784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./cifar-10n-png/train/image_id_0.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./cifar-10n-png/train/image_id_1.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./cifar-10n-png/train/image_id_2.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./cifar-10n-png/train/image_id_3.png</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./cifar-10n-png/train/image_id_4.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image  label\n",
       "0  ./cifar-10n-png/train/image_id_0.png      4\n",
       "1  ./cifar-10n-png/train/image_id_1.png      9\n",
       "2  ./cifar-10n-png/train/image_id_2.png      0\n",
       "3  ./cifar-10n-png/train/image_id_3.png      5\n",
       "4  ./cifar-10n-png/train/image_id_4.png      1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c42f98-2cd1-4732-a967-e6734ae1b8f2",
   "metadata": {},
   "source": [
    "# Optional: copy the images to a faster drive (e.g. M.2 SSD) which will speedup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7a433c8-3b1b-4142-9d7f-703e63370d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp -r ./cifar-10n-png/ /Data/cifar-10n-png/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90ce8831-7ee0-4751-b8cb-8c70b2431f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /Data/cifar-10n-png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12372664-99ab-4430-841c-17ff7899d12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Data/cifar-10n-png/train/image_id_0.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Data/cifar-10n-png/train/image_id_1.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Data/cifar-10n-png/train/image_id_2.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Data/cifar-10n-png/train/image_id_3.png</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Data/cifar-10n-png/train/image_id_4.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      image  label\n",
       "0  /Data/cifar-10n-png/train/image_id_0.png      4\n",
       "1  /Data/cifar-10n-png/train/image_id_1.png      9\n",
       "2  /Data/cifar-10n-png/train/image_id_2.png      0\n",
       "3  /Data/cifar-10n-png/train/image_id_3.png      5\n",
       "4  /Data/cifar-10n-png/train/image_id_4.png      1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add prefix to file path if we moved it to another drive\n",
    "prefix = \"/Data/\"\n",
    "df_train[\"image\"] = df_train.image.map(lambda f: str(Path(prefix + f)))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c5ccf0a-0429-4491-92e6-492d62654a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image    /Data/cifar-10n-png/train/image_id_0.png\n",
       "label                                           4\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b1d78e3-85b0-4ca7-b2ef-16ed3cff5740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0bf22b7d\t\t\t       a3292b9f\t\t       e0705ad5\n",
      "331ca34a\t\t\t       a6b2c926\t\t       e5ab5b04\n",
      "53d0bfc3\t\t\t       cifar-10-100n\t       play.ipynb\n",
      "961e4305\t\t\t       cifar-10-100n-main.zip\n",
      "Prepare_CIFAR-10N_For_Autogluon.ipynb  cifar-10n-png\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b2120-a02c-464f-a6be-65ad59243c1b",
   "metadata": {},
   "source": [
    "## Run cross-validation with AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4d08cbe-06fa-4560-9bf7-e69bdb8fc0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Running cross-validation for model: resnet18\n",
      "----\n",
      "Running Cross-Validation on Split: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "modified configs(<old> != <new>): {\n",
      "root.img_cls.model   resnet101 != resnet18\n",
      "root.train.early_stop_max_value 1.0 != inf\n",
      "root.train.early_stop_baseline 0.0 != -inf\n",
      "root.train.early_stop_patience -1 != 10\n",
      "root.train.batch_size 32 != 16\n",
      "root.train.epochs    200 != 100\n",
      "root.misc.seed       42 != 660\n",
      "root.misc.num_workers 4 != 16\n",
      "}\n",
      "Saved config to /dcai/src/experiments/cifar-n/a6b2c926/.trial_0/config.yaml\n",
      "Model resnet18 created, param count:                                         11181642\n",
      "AMP not enabled. Training in float32.\n",
      "Disable EMA as it is not supported for now.\n",
      "Start training from [Epoch 0]\n",
      "Epoch[0] Batch [49]\tSpeed: 232.370721 samples/sec\taccuracy=0.126250\tlr=0.000100\n",
      "Epoch[0] Batch [99]\tSpeed: 515.538348 samples/sec\taccuracy=0.125625\tlr=0.000100\n",
      "Epoch[0] Batch [149]\tSpeed: 531.036058 samples/sec\taccuracy=0.137917\tlr=0.000100\n",
      "Epoch[0] Batch [199]\tSpeed: 512.810578 samples/sec\taccuracy=0.145313\tlr=0.000100\n",
      "Epoch[0] Batch [249]\tSpeed: 493.327027 samples/sec\taccuracy=0.154500\tlr=0.000100\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error happened during fit: { 'args': \"{'img_cls': {'model': 'resnet18', 'pretrained': True, \"\n          \"'global_pool_type': None}, 'data': {'img_size': None, 'input_size': \"\n          \"None, 'crop_pct': 0.99, 'mean': None, 'std': None, 'interpolation': \"\n          \"'', 'validation_batch_size_multiplier': 1}, 'optimizer': {'opt': \"\n          \"'sgd', 'opt_eps': None, 'opt_betas': None, 'momentum': 0.9, \"\n          \"'weight_decay': 0.0001, 'clip_grad': None, 'clip_mode': 'norm'}, \"\n          \"'train': {'batch_size': 16, 'sched': 'step', 'lr': 0.01, \"\n          \"'lr_noise': None, 'lr_noise_pct': 0.67, 'lr_noise_std': 1.0, \"\n          \"'lr_cycle_mul': 1.0, 'lr_cycle_limit': 1, 'transfer_lr_mult': 0.01, \"\n          \"'output_lr_mult': 0.1, 'warmup_lr': 0.0001, 'min_lr': 1e-05, \"\n          \"'epochs': 100, 'start_epoch': 0, 'decay_epochs': 30, \"\n          \"'warmup_epochs': 3, 'cooldown_epochs': 10, 'patience_epochs': 10, \"\n          \"'decay_rate': 0.1, 'bn_momentum': None, 'bn_eps': None, 'sync_bn': \"\n          \"False, 'early_stop_patience': 10, 'early_stop_min_delta': 0.001, \"\n          \"'early_stop_baseline': -inf, 'early_stop_max_value': inf}, \"\n          \"'augmentation': {'no_aug': False, 'scale': (0.08, 1.0), 'ratio': \"\n          \"(0.75, 1.3333333333333333), 'hflip': 0.5, 'vflip': 0.0, \"\n          \"'color_jitter': 0.4, 'auto_augment': None, 'mixup': 0.0, 'cutmix': \"\n          \"0.0, 'cutmix_minmax': None, 'mixup_prob': 1.0, 'mixup_switch_prob': \"\n          \"0.5, 'mixup_mode': 'batch', 'mixup_off_epoch': 0, 'smoothing': 0.1, \"\n          \"'train_interpolation': 'random', 'drop': 0.0, 'drop_path': None, \"\n          \"'drop_block': None}, 'model_ema': {'model_ema': True, \"\n          \"'model_ema_force_cpu': False, 'model_ema_decay': 0.9998}, 'misc': \"\n          \"{'seed': 660, 'log_interval': 50, 'num_workers': 16, 'save_images': \"\n          \"False, 'amp': False, 'apex_amp': False, 'native_amp': False, \"\n          \"'pin_mem': False, 'prefetcher': False, 'eval_metric': 'top1', \"\n          \"'tta': 0, 'use_multi_epochs_loader': False, 'torchscript': False}, \"\n          \"'gpus': [0]}\",\n  'time': 10.478505373001099,\n  'traceback': 'Traceback (most recent call last):\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/tasks/image_classification.py\", '\n               'line 195, in _train_image_classification\\n'\n               '    result = estimator.fit(train_data=train_data, '\n               'val_data=val_data, time_limit=wall_clock_tick-tic)\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/estimators/base_estimator.py\", '\n               'line 175, in fit\\n'\n               '    ret = self._fit(train_data, val_data, '\n               'time_limit=time_limit) if not resume else \\\\\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/estimators/torch_image_classification/torch_image_classification.py\", '\n               'line 128, in _fit\\n'\n               '    return self._resume_fit(train_data, val_data, '\n               'time_limit=time_limit)\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/estimators/torch_image_classification/torch_image_classification.py\", '\n               'line 206, in _resume_fit\\n'\n               '    return self._train_loop(train_loader, val_loader, '\n               'time_limit=time_limit)\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/estimators/torch_image_classification/torch_image_classification.py\", '\n               'line 244, in _train_loop\\n'\n               '    train_metrics = self.train_one_epoch(\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/estimators/torch_image_classification/torch_image_classification.py\", '\n               'line 367, in train_one_epoch\\n'\n               '    loss.backward(create_graph=second_order)\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", '\n               'line 307, in backward\\n'\n               '    torch.autograd.backward(self, gradient, retain_graph, '\n               'create_graph, inputs=inputs)\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", '\n               'line 154, in backward\\n'\n               '    Variable._execution_engine.run_backward(\\n'\n               'KeyboardInterrupt\\n',\n  'train_acc': -1,\n  'valid_acc': -1}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/dcai/src/experiments/cifar-n/../cross_validation_autogluon.py:73\u001b[0m, in \u001b[0;36mcross_val_predict_autogluon_image_dataset\u001b[0;34m(dataset, out_folder, n_splits, model_params, ngpus_per_trial, time_limit, random_state)\u001b[0m\n\u001b[1;32m     70\u001b[0m predictor \u001b[38;5;241m=\u001b[39m ImagePredictor(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# train model on train indices in this split\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mngpus_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mngpus_per_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# predict on test indices in this split\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# predicted probabilities for test split\u001b[39;00m\n\u001b[1;32m     84\u001b[0m pred_probs \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mpredict_proba(\n\u001b[1;32m     85\u001b[0m     data\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39miloc[test_index], as_pandas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     86\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/autogluon/vision/configs/presets_configs.py:18\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     17\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m set_presets(preset_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/autogluon/vision/predictor/predictor.py:421\u001b[0m, in \u001b[0;36mImagePredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_classes \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mclasses\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m w:\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;66;03m# TODO: MXNetErrorCatcher was removed because it didn't return traceback\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m#  Re-add once it returns full traceback regardless of which exception was caught\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtuning_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classifier\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39msetLevel(log_level)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classifier\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mpropagate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gluoncv/auto/tasks/image_classification.py:444\u001b[0m, in \u001b[0;36mImageClassification.fit\u001b[0;34m(self, train_data, val_data, train_size, random_state, time_limit)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to fit a usable model given `time_limit=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_limit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected error happened during fit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpprint\u001b[38;5;241m.\u001b[39mpformat(results, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    446\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mloads(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error happened during fit: { 'args': \"{'img_cls': {'model': 'resnet18', 'pretrained': True, \"\n          \"'global_pool_type': None}, 'data': {'img_size': None, 'input_size': \"\n          \"None, 'crop_pct': 0.99, 'mean': None, 'std': None, 'interpolation': \"\n          \"'', 'validation_batch_size_multiplier': 1}, 'optimizer': {'opt': \"\n          \"'sgd', 'opt_eps': None, 'opt_betas': None, 'momentum': 0.9, \"\n          \"'weight_decay': 0.0001, 'clip_grad': None, 'clip_mode': 'norm'}, \"\n          \"'train': {'batch_size': 16, 'sched': 'step', 'lr': 0.01, \"\n          \"'lr_noise': None, 'lr_noise_pct': 0.67, 'lr_noise_std': 1.0, \"\n          \"'lr_cycle_mul': 1.0, 'lr_cycle_limit': 1, 'transfer_lr_mult': 0.01, \"\n          \"'output_lr_mult': 0.1, 'warmup_lr': 0.0001, 'min_lr': 1e-05, \"\n          \"'epochs': 100, 'start_epoch': 0, 'decay_epochs': 30, \"\n          \"'warmup_epochs': 3, 'cooldown_epochs': 10, 'patience_epochs': 10, \"\n          \"'decay_rate': 0.1, 'bn_momentum': None, 'bn_eps': None, 'sync_bn': \"\n          \"False, 'early_stop_patience': 10, 'early_stop_min_delta': 0.001, \"\n          \"'early_stop_baseline': -inf, 'early_stop_max_value': inf}, \"\n          \"'augmentation': {'no_aug': False, 'scale': (0.08, 1.0), 'ratio': \"\n          \"(0.75, 1.3333333333333333), 'hflip': 0.5, 'vflip': 0.0, \"\n          \"'color_jitter': 0.4, 'auto_augment': None, 'mixup': 0.0, 'cutmix': \"\n          \"0.0, 'cutmix_minmax': None, 'mixup_prob': 1.0, 'mixup_switch_prob': \"\n          \"0.5, 'mixup_mode': 'batch', 'mixup_off_epoch': 0, 'smoothing': 0.1, \"\n          \"'train_interpolation': 'random', 'drop': 0.0, 'drop_path': None, \"\n          \"'drop_block': None}, 'model_ema': {'model_ema': True, \"\n          \"'model_ema_force_cpu': False, 'model_ema_decay': 0.9998}, 'misc': \"\n          \"{'seed': 660, 'log_interval': 50, 'num_workers': 16, 'save_images': \"\n          \"False, 'amp': False, 'apex_amp': False, 'native_amp': False, \"\n          \"'pin_mem': False, 'prefetcher': False, 'eval_metric': 'top1', \"\n          \"'tta': 0, 'use_multi_epochs_loader': False, 'torchscript': False}, \"\n          \"'gpus': [0]}\",\n  'time': 10.478505373001099,\n  'traceback': 'Traceback (most recent call last):\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/tasks/image_classification.py\", '\n               'line 195, in _train_image_classification\\n'\n               '    result = estimator.fit(train_data=train_data, '\n               'val_data=val_data, time_limit=wall_clock_tick-tic)\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/estimators/base_estimator.py\", '\n               'line 175, in fit\\n'\n               '    ret = self._fit(train_data, val_data, '\n               'time_limit=time_limit) if not resume else \\\\\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/estimators/torch_image_classification/torch_image_classification.py\", '\n               'line 128, in _fit\\n'\n               '    return self._resume_fit(train_data, val_data, '\n               'time_limit=time_limit)\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/estimators/torch_image_classification/torch_image_classification.py\", '\n               'line 206, in _resume_fit\\n'\n               '    return self._train_loop(train_loader, val_loader, '\n               'time_limit=time_limit)\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/estimators/torch_image_classification/torch_image_classification.py\", '\n               'line 244, in _train_loop\\n'\n               '    train_metrics = self.train_one_epoch(\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/gluoncv/auto/estimators/torch_image_classification/torch_image_classification.py\", '\n               'line 367, in train_one_epoch\\n'\n               '    loss.backward(create_graph=second_order)\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", '\n               'line 307, in backward\\n'\n               '    torch.autograd.backward(self, gradient, retain_graph, '\n               'create_graph, inputs=inputs)\\n'\n               '  File '\n               '\"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", '\n               'line 154, in backward\\n'\n               '    Variable._execution_engine.run_backward(\\n'\n               'KeyboardInterrupt\\n',\n  'train_acc': -1,\n  'valid_acc': -1}"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# generate cross-validated predicted probabilities for various models so we can use them for ensemble scoring methods\n",
    "models = [\n",
    "    \"resnet18\",\n",
    "    \"resnet50d\",\n",
    "    \"efficientnet_b1\",\n",
    "    \"twins_pcpvt_base\",\n",
    "    \"swin_base_patch4_window7_224\"\n",
    "]\n",
    "\n",
    "epochs = 100\n",
    "holdout_frac = 0.2\n",
    "n_splits = 5\n",
    "\n",
    "# run cross-validation for each model\n",
    "for model in models:\n",
    "    \n",
    "    print(\"----\")\n",
    "    print(f\"Running cross-validation for model: {model}\")\n",
    "\n",
    "    MODEL_PARAMS = {\n",
    "        \"model\": model,\n",
    "        \"epochs\": epochs,\n",
    "        \"holdout_frac\": holdout_frac\n",
    "    }\n",
    "\n",
    "    # results of cross-validation will be saved to pickle files for each model/fold\n",
    "    _ = \\\n",
    "        cross_val_predict_autogluon_image_dataset(\n",
    "            dataset=df_train, # train with NOISY LABELS\n",
    "            out_folder=f\"./cifar-10n-png_noise_type_worst_cv_{model}/\", # save results of cross-validation in pickle files for each fold\n",
    "            n_splits=n_splits,\n",
    "            model_params=MODEL_PARAMS\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
