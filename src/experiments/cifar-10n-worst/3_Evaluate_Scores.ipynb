{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "556fe131-ed4e-4edd-8880-634f997282ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import copy\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from eval_metrics import lift_at_k\n",
    "\n",
    "from cleanlab.rank import order_label_issues, get_label_quality_scores, get_label_quality_ensemble_scores\n",
    "from cleanlab.filter import find_label_issues\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d240f-7df9-4547-90cd-209f64877afb",
   "metadata": {},
   "source": [
    "## Evaluate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59edc254-a872-48f4-af92-00e5dfbc9c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 27.7 ms, total: 28.6 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "models = [\n",
    "    \"resnet18\", \n",
    "    \"resnet50d\",\n",
    "    \"efficientnet_b1\",\n",
    "    \"twins_pcpvt_base\",\n",
    "    \"swin_base_patch4_window7_224\"\n",
    "]\n",
    "\n",
    "# args to pass to get_label_quality_scores()\n",
    "score_params = \\\n",
    "[\n",
    "    (\"self_confidence\", False),\n",
    "    (\"self_confidence\", True),\n",
    "    (\"normalized_margin\", False),\n",
    "    (\"normalized_margin\", True),\n",
    "    (\"confidence_weighted_entropy\", False)\n",
    "]\n",
    "\n",
    "results_list = []\n",
    "pred_probs_list = [] # use for ensemble scoring\n",
    "labels_list = [] # use for sanity check (labels from each model should be the same because they were generated from the same cross-val procedure\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    # read numpy files\n",
    "    numpy_out_folder = f\"./cifar-10n-png_noise_type_worst_cv_{model}/\"\n",
    "    pred_probs = np.load(numpy_out_folder + \"pred_probs.npy\")\n",
    "    labels = np.load(numpy_out_folder + \"noisy_labels.npy\")\n",
    "    true_labels = np.load(numpy_out_folder + \"true_labels.npy\")\n",
    "    images = np.load(numpy_out_folder + \"images.npy\", allow_pickle=True)\n",
    "    \n",
    "    # boolean mask of label errors\n",
    "    label_errors_target = labels != true_labels\n",
    "    \n",
    "    # save to list for ensemble scoring\n",
    "    pred_probs_list.append(pred_probs)\n",
    "    \n",
    "    labels_list.append(labels)\n",
    "    \n",
    "    for score_param in score_params:\n",
    "        \n",
    "        method, adjust_pred_probs = score_param\n",
    "\n",
    "        # compute scores\n",
    "        label_quality_scores = get_label_quality_scores(labels=labels, pred_probs=pred_probs, method=method, adjust_pred_probs=adjust_pred_probs)\n",
    "\n",
    "        # compute accuracy of detecting label errors\n",
    "        auroc = roc_auc_score(label_errors_target, 1 - label_quality_scores)\n",
    "\n",
    "        # compute Lift@K evaluation metric\n",
    "        lift_at_k_dict = {}\n",
    "        for k in range(1000, 21000, 1000):\n",
    "            lift_at_k_dict[f\"lift_at_{k}\"] = lift_at_k(label_errors_target, 1 - label_quality_scores, k=k)\n",
    "\n",
    "        # save results\n",
    "        results = {\n",
    "            \"dataset\": \"cifar-10n\",\n",
    "            \"model\": model,\n",
    "            \"noise_type\": \"worse_label\",\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"auroc\": auroc\n",
    "        }\n",
    "\n",
    "        # add the lift at k metrics\n",
    "        results.update(lift_at_k_dict)\n",
    "\n",
    "        # save results\n",
    "        results_list.append(results)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cdd9b7d-9dc7-4800-9e99-8f64acbc20d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for sanity check (noisy labels from each model should be the same because they were generated from the same cross-val procedure\n",
    "for i, labels_temp in enumerate(labels_list):\n",
    "    \n",
    "    if i == 0:\n",
    "        # labels_temp_previous = labels_temp.copy()\n",
    "        labels_temp_previous = copy.deepcopy(labels_temp)\n",
    "    else:\n",
    "        assert (labels_temp_previous == labels_temp).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fcdf2f-0fc3-4449-994b-5044f46d2ba5",
   "metadata": {},
   "source": [
    "## Evaluate ensemble scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5bf03fd-25b3-42b2-a745-95c7c1aef9d3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring label quality...\n",
      "  method: self_confidence\n",
      "  adjust_pred_probs: False\n",
      "Weighting scheme for ensemble: accuracy\n",
      "Ensemble members will be weighted by: their relative accuracy\n",
      "  Model 0 accuracy : 0.57788\n",
      "  Model 0 weights  : 0.19538551006883867\n",
      "  Model 1 accuracy : 0.5919\n",
      "  Model 1 weights  : 0.20012577595650588\n",
      "  Model 2 accuracy : 0.5804\n",
      "  Model 2 weights  : 0.19623754074194294\n",
      "  Model 3 accuracy : 0.60154\n",
      "  Model 3 weights  : 0.20338513138853953\n",
      "  Model 4 accuracy : 0.60592\n",
      "  Model 4 weights  : 0.2048660418441731\n",
      "Scoring label quality...\n",
      "  method: self_confidence\n",
      "  adjust_pred_probs: True\n",
      "Weighting scheme for ensemble: accuracy\n",
      "Ensemble members will be weighted by: their relative accuracy\n",
      "  Model 0 accuracy : 0.57788\n",
      "  Model 0 weights  : 0.19538551006883867\n",
      "  Model 1 accuracy : 0.5919\n",
      "  Model 1 weights  : 0.20012577595650588\n",
      "  Model 2 accuracy : 0.5804\n",
      "  Model 2 weights  : 0.19623754074194294\n",
      "  Model 3 accuracy : 0.60154\n",
      "  Model 3 weights  : 0.20338513138853953\n",
      "  Model 4 accuracy : 0.60592\n",
      "  Model 4 weights  : 0.2048660418441731\n",
      "Scoring label quality...\n",
      "  method: normalized_margin\n",
      "  adjust_pred_probs: False\n",
      "Weighting scheme for ensemble: accuracy\n",
      "Ensemble members will be weighted by: their relative accuracy\n",
      "  Model 0 accuracy : 0.57788\n",
      "  Model 0 weights  : 0.19538551006883867\n",
      "  Model 1 accuracy : 0.5919\n",
      "  Model 1 weights  : 0.20012577595650588\n",
      "  Model 2 accuracy : 0.5804\n",
      "  Model 2 weights  : 0.19623754074194294\n",
      "  Model 3 accuracy : 0.60154\n",
      "  Model 3 weights  : 0.20338513138853953\n",
      "  Model 4 accuracy : 0.60592\n",
      "  Model 4 weights  : 0.2048660418441731\n",
      "Scoring label quality...\n",
      "  method: normalized_margin\n",
      "  adjust_pred_probs: True\n",
      "Weighting scheme for ensemble: accuracy\n",
      "Ensemble members will be weighted by: their relative accuracy\n",
      "  Model 0 accuracy : 0.57788\n",
      "  Model 0 weights  : 0.19538551006883867\n",
      "  Model 1 accuracy : 0.5919\n",
      "  Model 1 weights  : 0.20012577595650588\n",
      "  Model 2 accuracy : 0.5804\n",
      "  Model 2 weights  : 0.19623754074194294\n",
      "  Model 3 accuracy : 0.60154\n",
      "  Model 3 weights  : 0.20338513138853953\n",
      "  Model 4 accuracy : 0.60592\n",
      "  Model 4 weights  : 0.2048660418441731\n",
      "Scoring label quality...\n",
      "  method: confidence_weighted_entropy\n",
      "  adjust_pred_probs: False\n",
      "Weighting scheme for ensemble: accuracy\n",
      "Ensemble members will be weighted by: their relative accuracy\n",
      "  Model 0 accuracy : 0.57788\n",
      "  Model 0 weights  : 0.19538551006883867\n",
      "  Model 1 accuracy : 0.5919\n",
      "  Model 1 weights  : 0.20012577595650588\n",
      "  Model 2 accuracy : 0.5804\n",
      "  Model 2 weights  : 0.19623754074194294\n",
      "  Model 3 accuracy : 0.60154\n",
      "  Model 3 weights  : 0.20338513138853953\n",
      "  Model 4 accuracy : 0.60592\n",
      "  Model 4 weights  : 0.2048660418441731\n",
      "CPU times: user 26.7 s, sys: 31.5 ms, total: 26.8 s\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for score_param in score_params:\n",
    "\n",
    "    method, adjust_pred_probs = score_param\n",
    "    \n",
    "    print(f\"Scoring label quality...\")\n",
    "    print(f\"  method: {method}\")\n",
    "    print(f\"  adjust_pred_probs: {adjust_pred_probs}\")\n",
    "\n",
    "    label_quality_ensemble_scores = get_label_quality_ensemble_scores(labels, pred_probs_list, method=method, adjust_pred_probs=adjust_pred_probs)\n",
    "    \n",
    "    # compute accuracy of detecting label errors\n",
    "    auroc = roc_auc_score(label_errors_target, 1 - label_quality_ensemble_scores)\n",
    "\n",
    "    # compute Lift@K evaluation metric\n",
    "    lift_at_k_dict = {}\n",
    "    for k in range(1000, 21000, 1000):\n",
    "        lift_at_k_dict[f\"lift_at_{k}\"] = lift_at_k(label_errors_target, 1 - label_quality_ensemble_scores, k=k)\n",
    "\n",
    "    # save results\n",
    "    results = {\n",
    "        \"dataset\": \"cifar-10n\",\n",
    "        \"model\": \"ensemble (all)\",\n",
    "        \"noise_type\": \"worse_label\",\n",
    "        \"method\": method,\n",
    "        \"adjust_pred_probs\": adjust_pred_probs,\n",
    "        \"auroc\": auroc\n",
    "    }\n",
    "\n",
    "    # add the lift at k metrics\n",
    "    results.update(lift_at_k_dict)\n",
    "    \n",
    "    # save results\n",
    "    results_list.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e2865-09a2-4fce-8492-461beaf73220",
   "metadata": {},
   "source": [
    "## Create DataFrame with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfda7837-7369-46af-a586-70953302f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7488497a-3c9b-4a0a-afda-5056e954ab44",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>noise_type</th>\n",
       "      <th>method</th>\n",
       "      <th>adjust_pred_probs</th>\n",
       "      <th>auroc</th>\n",
       "      <th>lift_at_1000</th>\n",
       "      <th>lift_at_2000</th>\n",
       "      <th>lift_at_3000</th>\n",
       "      <th>lift_at_4000</th>\n",
       "      <th>lift_at_5000</th>\n",
       "      <th>lift_at_6000</th>\n",
       "      <th>lift_at_7000</th>\n",
       "      <th>lift_at_8000</th>\n",
       "      <th>lift_at_9000</th>\n",
       "      <th>lift_at_10000</th>\n",
       "      <th>lift_at_11000</th>\n",
       "      <th>lift_at_12000</th>\n",
       "      <th>lift_at_13000</th>\n",
       "      <th>lift_at_14000</th>\n",
       "      <th>lift_at_15000</th>\n",
       "      <th>lift_at_16000</th>\n",
       "      <th>lift_at_17000</th>\n",
       "      <th>lift_at_18000</th>\n",
       "      <th>lift_at_19000</th>\n",
       "      <th>lift_at_20000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>False</td>\n",
       "      <td>0.963435</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.485202</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.482508</td>\n",
       "      <td>2.479606</td>\n",
       "      <td>2.474321</td>\n",
       "      <td>2.469381</td>\n",
       "      <td>2.460953</td>\n",
       "      <td>2.449987</td>\n",
       "      <td>2.435668</td>\n",
       "      <td>2.418386</td>\n",
       "      <td>2.396645</td>\n",
       "      <td>2.372165</td>\n",
       "      <td>2.338465</td>\n",
       "      <td>2.307852</td>\n",
       "      <td>2.267929</td>\n",
       "      <td>2.226972</td>\n",
       "      <td>2.178547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>True</td>\n",
       "      <td>0.956146</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.470901</td>\n",
       "      <td>2.453077</td>\n",
       "      <td>2.439813</td>\n",
       "      <td>2.431357</td>\n",
       "      <td>2.433595</td>\n",
       "      <td>2.433062</td>\n",
       "      <td>2.427067</td>\n",
       "      <td>2.422127</td>\n",
       "      <td>2.417927</td>\n",
       "      <td>2.406577</td>\n",
       "      <td>2.395046</td>\n",
       "      <td>2.377636</td>\n",
       "      <td>2.357917</td>\n",
       "      <td>2.336185</td>\n",
       "      <td>2.306755</td>\n",
       "      <td>2.275228</td>\n",
       "      <td>2.238913</td>\n",
       "      <td>2.200138</td>\n",
       "      <td>2.153552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>False</td>\n",
       "      <td>0.966803</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.482093</td>\n",
       "      <td>2.482093</td>\n",
       "      <td>2.477119</td>\n",
       "      <td>2.472642</td>\n",
       "      <td>2.468414</td>\n",
       "      <td>2.462552</td>\n",
       "      <td>2.457533</td>\n",
       "      <td>2.452525</td>\n",
       "      <td>2.446777</td>\n",
       "      <td>2.435291</td>\n",
       "      <td>2.423440</td>\n",
       "      <td>2.411499</td>\n",
       "      <td>2.396289</td>\n",
       "      <td>2.379129</td>\n",
       "      <td>2.357895</td>\n",
       "      <td>2.328041</td>\n",
       "      <td>2.293767</td>\n",
       "      <td>2.252628</td>\n",
       "      <td>2.202795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>True</td>\n",
       "      <td>0.966067</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.479606</td>\n",
       "      <td>2.476290</td>\n",
       "      <td>2.473388</td>\n",
       "      <td>2.469658</td>\n",
       "      <td>2.465098</td>\n",
       "      <td>2.461486</td>\n",
       "      <td>2.456290</td>\n",
       "      <td>2.451972</td>\n",
       "      <td>2.445036</td>\n",
       "      <td>2.433708</td>\n",
       "      <td>2.421574</td>\n",
       "      <td>2.409203</td>\n",
       "      <td>2.393802</td>\n",
       "      <td>2.372994</td>\n",
       "      <td>2.347791</td>\n",
       "      <td>2.319995</td>\n",
       "      <td>2.286582</td>\n",
       "      <td>2.247654</td>\n",
       "      <td>2.201925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>confidence_weighted_entropy</td>\n",
       "      <td>False</td>\n",
       "      <td>0.954042</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.482093</td>\n",
       "      <td>2.480850</td>\n",
       "      <td>2.480601</td>\n",
       "      <td>2.477948</td>\n",
       "      <td>2.473566</td>\n",
       "      <td>2.466549</td>\n",
       "      <td>2.457499</td>\n",
       "      <td>2.444787</td>\n",
       "      <td>2.430091</td>\n",
       "      <td>2.411833</td>\n",
       "      <td>2.390454</td>\n",
       "      <td>2.360049</td>\n",
       "      <td>2.331377</td>\n",
       "      <td>2.294630</td>\n",
       "      <td>2.257964</td>\n",
       "      <td>2.220675</td>\n",
       "      <td>2.174613</td>\n",
       "      <td>2.127064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>resnet50d</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>False</td>\n",
       "      <td>0.975780</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.485409</td>\n",
       "      <td>2.485824</td>\n",
       "      <td>2.485575</td>\n",
       "      <td>2.483751</td>\n",
       "      <td>2.482448</td>\n",
       "      <td>2.480539</td>\n",
       "      <td>2.477119</td>\n",
       "      <td>2.472394</td>\n",
       "      <td>2.468075</td>\n",
       "      <td>2.459088</td>\n",
       "      <td>2.445361</td>\n",
       "      <td>2.426845</td>\n",
       "      <td>2.404828</td>\n",
       "      <td>2.379035</td>\n",
       "      <td>2.354960</td>\n",
       "      <td>2.325546</td>\n",
       "      <td>2.291767</td>\n",
       "      <td>2.249925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>resnet50d</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>True</td>\n",
       "      <td>0.971084</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.458466</td>\n",
       "      <td>2.458051</td>\n",
       "      <td>2.460331</td>\n",
       "      <td>2.460207</td>\n",
       "      <td>2.458880</td>\n",
       "      <td>2.453669</td>\n",
       "      <td>2.451937</td>\n",
       "      <td>2.451419</td>\n",
       "      <td>2.448766</td>\n",
       "      <td>2.444335</td>\n",
       "      <td>2.438569</td>\n",
       "      <td>2.425273</td>\n",
       "      <td>2.409258</td>\n",
       "      <td>2.386921</td>\n",
       "      <td>2.360227</td>\n",
       "      <td>2.333454</td>\n",
       "      <td>2.304820</td>\n",
       "      <td>2.270823</td>\n",
       "      <td>2.233759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>resnet50d</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>False</td>\n",
       "      <td>0.979797</td>\n",
       "      <td>2.482093</td>\n",
       "      <td>2.480850</td>\n",
       "      <td>2.479606</td>\n",
       "      <td>2.477741</td>\n",
       "      <td>2.475129</td>\n",
       "      <td>2.472559</td>\n",
       "      <td>2.469658</td>\n",
       "      <td>2.468414</td>\n",
       "      <td>2.466342</td>\n",
       "      <td>2.464684</td>\n",
       "      <td>2.460840</td>\n",
       "      <td>2.456393</td>\n",
       "      <td>2.448613</td>\n",
       "      <td>2.441057</td>\n",
       "      <td>2.427212</td>\n",
       "      <td>2.413543</td>\n",
       "      <td>2.396362</td>\n",
       "      <td>2.366168</td>\n",
       "      <td>2.330382</td>\n",
       "      <td>2.284620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>resnet50d</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>True</td>\n",
       "      <td>0.980028</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.479606</td>\n",
       "      <td>2.477119</td>\n",
       "      <td>2.477119</td>\n",
       "      <td>2.473637</td>\n",
       "      <td>2.471730</td>\n",
       "      <td>2.471079</td>\n",
       "      <td>2.468103</td>\n",
       "      <td>2.464407</td>\n",
       "      <td>2.462197</td>\n",
       "      <td>2.457901</td>\n",
       "      <td>2.455772</td>\n",
       "      <td>2.451674</td>\n",
       "      <td>2.439991</td>\n",
       "      <td>2.427875</td>\n",
       "      <td>2.413232</td>\n",
       "      <td>2.394314</td>\n",
       "      <td>2.365063</td>\n",
       "      <td>2.329989</td>\n",
       "      <td>2.284869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>resnet50d</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>confidence_weighted_entropy</td>\n",
       "      <td>False</td>\n",
       "      <td>0.968417</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.485824</td>\n",
       "      <td>2.486238</td>\n",
       "      <td>2.483958</td>\n",
       "      <td>2.482591</td>\n",
       "      <td>2.480850</td>\n",
       "      <td>2.477830</td>\n",
       "      <td>2.474321</td>\n",
       "      <td>2.471592</td>\n",
       "      <td>2.466176</td>\n",
       "      <td>2.458805</td>\n",
       "      <td>2.448310</td>\n",
       "      <td>2.433691</td>\n",
       "      <td>2.411745</td>\n",
       "      <td>2.377968</td>\n",
       "      <td>2.345149</td>\n",
       "      <td>2.310924</td>\n",
       "      <td>2.278154</td>\n",
       "      <td>2.242549</td>\n",
       "      <td>2.204909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>efficientnet_b1</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>False</td>\n",
       "      <td>0.964984</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.483751</td>\n",
       "      <td>2.483337</td>\n",
       "      <td>2.482093</td>\n",
       "      <td>2.478363</td>\n",
       "      <td>2.477474</td>\n",
       "      <td>2.474321</td>\n",
       "      <td>2.471039</td>\n",
       "      <td>2.463689</td>\n",
       "      <td>2.448179</td>\n",
       "      <td>2.434424</td>\n",
       "      <td>2.414177</td>\n",
       "      <td>2.388650</td>\n",
       "      <td>2.359729</td>\n",
       "      <td>2.333957</td>\n",
       "      <td>2.302439</td>\n",
       "      <td>2.267376</td>\n",
       "      <td>2.226449</td>\n",
       "      <td>2.186256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>efficientnet_b1</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>True</td>\n",
       "      <td>0.957359</td>\n",
       "      <td>2.477119</td>\n",
       "      <td>2.458466</td>\n",
       "      <td>2.447274</td>\n",
       "      <td>2.441678</td>\n",
       "      <td>2.437823</td>\n",
       "      <td>2.429450</td>\n",
       "      <td>2.424180</td>\n",
       "      <td>2.421160</td>\n",
       "      <td>2.416048</td>\n",
       "      <td>2.411460</td>\n",
       "      <td>2.405898</td>\n",
       "      <td>2.393180</td>\n",
       "      <td>2.378210</td>\n",
       "      <td>2.357029</td>\n",
       "      <td>2.333366</td>\n",
       "      <td>2.300537</td>\n",
       "      <td>2.268498</td>\n",
       "      <td>2.237117</td>\n",
       "      <td>2.198567</td>\n",
       "      <td>2.158401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>efficientnet_b1</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>False</td>\n",
       "      <td>0.970835</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.482922</td>\n",
       "      <td>2.480850</td>\n",
       "      <td>2.478611</td>\n",
       "      <td>2.473388</td>\n",
       "      <td>2.465750</td>\n",
       "      <td>2.458777</td>\n",
       "      <td>2.453630</td>\n",
       "      <td>2.448518</td>\n",
       "      <td>2.441396</td>\n",
       "      <td>2.433388</td>\n",
       "      <td>2.421064</td>\n",
       "      <td>2.410679</td>\n",
       "      <td>2.393222</td>\n",
       "      <td>2.372973</td>\n",
       "      <td>2.347645</td>\n",
       "      <td>2.317256</td>\n",
       "      <td>2.280117</td>\n",
       "      <td>2.235127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>efficientnet_b1</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>True</td>\n",
       "      <td>0.970507</td>\n",
       "      <td>2.479606</td>\n",
       "      <td>2.483337</td>\n",
       "      <td>2.480435</td>\n",
       "      <td>2.476497</td>\n",
       "      <td>2.474135</td>\n",
       "      <td>2.470901</td>\n",
       "      <td>2.466105</td>\n",
       "      <td>2.460020</td>\n",
       "      <td>2.453630</td>\n",
       "      <td>2.447025</td>\n",
       "      <td>2.439361</td>\n",
       "      <td>2.430901</td>\n",
       "      <td>2.419151</td>\n",
       "      <td>2.407836</td>\n",
       "      <td>2.386756</td>\n",
       "      <td>2.368465</td>\n",
       "      <td>2.343988</td>\n",
       "      <td>2.311176</td>\n",
       "      <td>2.273834</td>\n",
       "      <td>2.228412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>efficientnet_b1</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>confidence_weighted_entropy</td>\n",
       "      <td>False</td>\n",
       "      <td>0.955172</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.482093</td>\n",
       "      <td>2.481264</td>\n",
       "      <td>2.479606</td>\n",
       "      <td>2.477119</td>\n",
       "      <td>2.474217</td>\n",
       "      <td>2.471790</td>\n",
       "      <td>2.468103</td>\n",
       "      <td>2.460262</td>\n",
       "      <td>2.447025</td>\n",
       "      <td>2.434839</td>\n",
       "      <td>2.413906</td>\n",
       "      <td>2.391411</td>\n",
       "      <td>2.357740</td>\n",
       "      <td>2.327729</td>\n",
       "      <td>2.291832</td>\n",
       "      <td>2.254307</td>\n",
       "      <td>2.216253</td>\n",
       "      <td>2.174875</td>\n",
       "      <td>2.133779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>twins_pcpvt_base</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>False</td>\n",
       "      <td>0.984722</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.486238</td>\n",
       "      <td>2.485824</td>\n",
       "      <td>2.486072</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.483870</td>\n",
       "      <td>2.483958</td>\n",
       "      <td>2.482922</td>\n",
       "      <td>2.481596</td>\n",
       "      <td>2.479606</td>\n",
       "      <td>2.477326</td>\n",
       "      <td>2.471380</td>\n",
       "      <td>2.461841</td>\n",
       "      <td>2.448103</td>\n",
       "      <td>2.431108</td>\n",
       "      <td>2.411285</td>\n",
       "      <td>2.388275</td>\n",
       "      <td>2.358394</td>\n",
       "      <td>2.318444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>twins_pcpvt_base</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>True</td>\n",
       "      <td>0.981895</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.472145</td>\n",
       "      <td>2.476290</td>\n",
       "      <td>2.475254</td>\n",
       "      <td>2.474632</td>\n",
       "      <td>2.475046</td>\n",
       "      <td>2.472500</td>\n",
       "      <td>2.468103</td>\n",
       "      <td>2.467171</td>\n",
       "      <td>2.466673</td>\n",
       "      <td>2.464458</td>\n",
       "      <td>2.463233</td>\n",
       "      <td>2.457222</td>\n",
       "      <td>2.445675</td>\n",
       "      <td>2.431854</td>\n",
       "      <td>2.414165</td>\n",
       "      <td>2.397825</td>\n",
       "      <td>2.373077</td>\n",
       "      <td>2.342556</td>\n",
       "      <td>2.301159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>twins_pcpvt_base</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>False</td>\n",
       "      <td>0.986910</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.482093</td>\n",
       "      <td>2.482715</td>\n",
       "      <td>2.482093</td>\n",
       "      <td>2.481679</td>\n",
       "      <td>2.480672</td>\n",
       "      <td>2.479295</td>\n",
       "      <td>2.477672</td>\n",
       "      <td>2.474881</td>\n",
       "      <td>2.473049</td>\n",
       "      <td>2.470487</td>\n",
       "      <td>2.466405</td>\n",
       "      <td>2.462374</td>\n",
       "      <td>2.456725</td>\n",
       "      <td>2.449917</td>\n",
       "      <td>2.437033</td>\n",
       "      <td>2.419226</td>\n",
       "      <td>2.386537</td>\n",
       "      <td>2.338092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>twins_pcpvt_base</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>True</td>\n",
       "      <td>0.987353</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.483337</td>\n",
       "      <td>2.482922</td>\n",
       "      <td>2.482715</td>\n",
       "      <td>2.483088</td>\n",
       "      <td>2.480435</td>\n",
       "      <td>2.479606</td>\n",
       "      <td>2.477741</td>\n",
       "      <td>2.477395</td>\n",
       "      <td>2.475875</td>\n",
       "      <td>2.473501</td>\n",
       "      <td>2.471316</td>\n",
       "      <td>2.468127</td>\n",
       "      <td>2.463262</td>\n",
       "      <td>2.457388</td>\n",
       "      <td>2.447274</td>\n",
       "      <td>2.434839</td>\n",
       "      <td>2.416462</td>\n",
       "      <td>2.382349</td>\n",
       "      <td>2.336848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>twins_pcpvt_base</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>confidence_weighted_entropy</td>\n",
       "      <td>False</td>\n",
       "      <td>0.980211</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.485824</td>\n",
       "      <td>2.486238</td>\n",
       "      <td>2.485202</td>\n",
       "      <td>2.485078</td>\n",
       "      <td>2.484995</td>\n",
       "      <td>2.483870</td>\n",
       "      <td>2.482093</td>\n",
       "      <td>2.480988</td>\n",
       "      <td>2.479109</td>\n",
       "      <td>2.476441</td>\n",
       "      <td>2.472145</td>\n",
       "      <td>2.465640</td>\n",
       "      <td>2.454202</td>\n",
       "      <td>2.433844</td>\n",
       "      <td>2.405616</td>\n",
       "      <td>2.377929</td>\n",
       "      <td>2.351246</td>\n",
       "      <td>2.321219</td>\n",
       "      <td>2.278154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>swin_base_patch4_window7_224</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>False</td>\n",
       "      <td>0.990451</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.486712</td>\n",
       "      <td>2.486135</td>\n",
       "      <td>2.485962</td>\n",
       "      <td>2.485326</td>\n",
       "      <td>2.484806</td>\n",
       "      <td>2.483129</td>\n",
       "      <td>2.481328</td>\n",
       "      <td>2.479606</td>\n",
       "      <td>2.472476</td>\n",
       "      <td>2.463285</td>\n",
       "      <td>2.453419</td>\n",
       "      <td>2.437188</td>\n",
       "      <td>2.412193</td>\n",
       "      <td>2.363709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>swin_base_patch4_window7_224</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>True</td>\n",
       "      <td>0.989189</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.485824</td>\n",
       "      <td>2.485409</td>\n",
       "      <td>2.483958</td>\n",
       "      <td>2.483585</td>\n",
       "      <td>2.482922</td>\n",
       "      <td>2.481027</td>\n",
       "      <td>2.479917</td>\n",
       "      <td>2.480711</td>\n",
       "      <td>2.480103</td>\n",
       "      <td>2.479832</td>\n",
       "      <td>2.478570</td>\n",
       "      <td>2.476736</td>\n",
       "      <td>2.473211</td>\n",
       "      <td>2.468497</td>\n",
       "      <td>2.458155</td>\n",
       "      <td>2.445080</td>\n",
       "      <td>2.429588</td>\n",
       "      <td>2.402245</td>\n",
       "      <td>2.356372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>swin_base_patch4_window7_224</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992099</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.486238</td>\n",
       "      <td>2.485202</td>\n",
       "      <td>2.485078</td>\n",
       "      <td>2.485409</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.484269</td>\n",
       "      <td>2.484304</td>\n",
       "      <td>2.483834</td>\n",
       "      <td>2.482998</td>\n",
       "      <td>2.480228</td>\n",
       "      <td>2.479032</td>\n",
       "      <td>2.478007</td>\n",
       "      <td>2.474300</td>\n",
       "      <td>2.471057</td>\n",
       "      <td>2.465123</td>\n",
       "      <td>2.452110</td>\n",
       "      <td>2.423189</td>\n",
       "      <td>2.376890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>swin_base_patch4_window7_224</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>True</td>\n",
       "      <td>0.992943</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.485575</td>\n",
       "      <td>2.484166</td>\n",
       "      <td>2.484225</td>\n",
       "      <td>2.483958</td>\n",
       "      <td>2.484304</td>\n",
       "      <td>2.483088</td>\n",
       "      <td>2.482545</td>\n",
       "      <td>2.481471</td>\n",
       "      <td>2.479797</td>\n",
       "      <td>2.478363</td>\n",
       "      <td>2.476290</td>\n",
       "      <td>2.472145</td>\n",
       "      <td>2.466147</td>\n",
       "      <td>2.454874</td>\n",
       "      <td>2.428687</td>\n",
       "      <td>2.382983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>swin_base_patch4_window7_224</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>confidence_weighted_entropy</td>\n",
       "      <td>False</td>\n",
       "      <td>0.986798</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.486072</td>\n",
       "      <td>2.486238</td>\n",
       "      <td>2.485291</td>\n",
       "      <td>2.485202</td>\n",
       "      <td>2.484028</td>\n",
       "      <td>2.483585</td>\n",
       "      <td>2.482771</td>\n",
       "      <td>2.482508</td>\n",
       "      <td>2.479797</td>\n",
       "      <td>2.473744</td>\n",
       "      <td>2.464849</td>\n",
       "      <td>2.449139</td>\n",
       "      <td>2.432644</td>\n",
       "      <td>2.409830</td>\n",
       "      <td>2.382087</td>\n",
       "      <td>2.335107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>ensemble (all)</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>False</td>\n",
       "      <td>0.989748</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.486357</td>\n",
       "      <td>2.486445</td>\n",
       "      <td>2.486238</td>\n",
       "      <td>2.485575</td>\n",
       "      <td>2.484806</td>\n",
       "      <td>2.483129</td>\n",
       "      <td>2.480945</td>\n",
       "      <td>2.475165</td>\n",
       "      <td>2.467337</td>\n",
       "      <td>2.452559</td>\n",
       "      <td>2.437911</td>\n",
       "      <td>2.417153</td>\n",
       "      <td>2.389417</td>\n",
       "      <td>2.344931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>ensemble (all)</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>self_confidence</td>\n",
       "      <td>True</td>\n",
       "      <td>0.987316</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.485409</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.484580</td>\n",
       "      <td>2.484166</td>\n",
       "      <td>2.483159</td>\n",
       "      <td>2.483337</td>\n",
       "      <td>2.482646</td>\n",
       "      <td>2.481098</td>\n",
       "      <td>2.479380</td>\n",
       "      <td>2.476912</td>\n",
       "      <td>2.473675</td>\n",
       "      <td>2.465927</td>\n",
       "      <td>2.455067</td>\n",
       "      <td>2.438725</td>\n",
       "      <td>2.422111</td>\n",
       "      <td>2.399191</td>\n",
       "      <td>2.367950</td>\n",
       "      <td>2.322796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>ensemble (all)</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992681</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.486001</td>\n",
       "      <td>2.486135</td>\n",
       "      <td>2.486238</td>\n",
       "      <td>2.485824</td>\n",
       "      <td>2.484354</td>\n",
       "      <td>2.483544</td>\n",
       "      <td>2.482476</td>\n",
       "      <td>2.480139</td>\n",
       "      <td>2.478777</td>\n",
       "      <td>2.474943</td>\n",
       "      <td>2.464537</td>\n",
       "      <td>2.448241</td>\n",
       "      <td>2.420178</td>\n",
       "      <td>2.374527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>ensemble (all)</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>normalized_margin</td>\n",
       "      <td>True</td>\n",
       "      <td>0.993323</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.486570</td>\n",
       "      <td>2.486238</td>\n",
       "      <td>2.486001</td>\n",
       "      <td>2.485824</td>\n",
       "      <td>2.485962</td>\n",
       "      <td>2.486072</td>\n",
       "      <td>2.485711</td>\n",
       "      <td>2.484373</td>\n",
       "      <td>2.483050</td>\n",
       "      <td>2.481027</td>\n",
       "      <td>2.478114</td>\n",
       "      <td>2.474787</td>\n",
       "      <td>2.464976</td>\n",
       "      <td>2.449623</td>\n",
       "      <td>2.423058</td>\n",
       "      <td>2.375771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cifar-10n</td>\n",
       "      <td>ensemble (all)</td>\n",
       "      <td>worse_label</td>\n",
       "      <td>confidence_weighted_entropy</td>\n",
       "      <td>False</td>\n",
       "      <td>0.984303</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.487067</td>\n",
       "      <td>2.486570</td>\n",
       "      <td>2.486238</td>\n",
       "      <td>2.486357</td>\n",
       "      <td>2.485824</td>\n",
       "      <td>2.484304</td>\n",
       "      <td>2.484083</td>\n",
       "      <td>2.482545</td>\n",
       "      <td>2.480021</td>\n",
       "      <td>2.474632</td>\n",
       "      <td>2.464861</td>\n",
       "      <td>2.447606</td>\n",
       "      <td>2.421315</td>\n",
       "      <td>2.396948</td>\n",
       "      <td>2.368655</td>\n",
       "      <td>2.335356</td>\n",
       "      <td>2.292579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset                         model   noise_type  \\\n",
       "0   cifar-10n                      resnet18  worse_label   \n",
       "1   cifar-10n                      resnet18  worse_label   \n",
       "2   cifar-10n                      resnet18  worse_label   \n",
       "3   cifar-10n                      resnet18  worse_label   \n",
       "4   cifar-10n                      resnet18  worse_label   \n",
       "5   cifar-10n                     resnet50d  worse_label   \n",
       "6   cifar-10n                     resnet50d  worse_label   \n",
       "7   cifar-10n                     resnet50d  worse_label   \n",
       "8   cifar-10n                     resnet50d  worse_label   \n",
       "9   cifar-10n                     resnet50d  worse_label   \n",
       "10  cifar-10n               efficientnet_b1  worse_label   \n",
       "11  cifar-10n               efficientnet_b1  worse_label   \n",
       "12  cifar-10n               efficientnet_b1  worse_label   \n",
       "13  cifar-10n               efficientnet_b1  worse_label   \n",
       "14  cifar-10n               efficientnet_b1  worse_label   \n",
       "15  cifar-10n              twins_pcpvt_base  worse_label   \n",
       "16  cifar-10n              twins_pcpvt_base  worse_label   \n",
       "17  cifar-10n              twins_pcpvt_base  worse_label   \n",
       "18  cifar-10n              twins_pcpvt_base  worse_label   \n",
       "19  cifar-10n              twins_pcpvt_base  worse_label   \n",
       "20  cifar-10n  swin_base_patch4_window7_224  worse_label   \n",
       "21  cifar-10n  swin_base_patch4_window7_224  worse_label   \n",
       "22  cifar-10n  swin_base_patch4_window7_224  worse_label   \n",
       "23  cifar-10n  swin_base_patch4_window7_224  worse_label   \n",
       "24  cifar-10n  swin_base_patch4_window7_224  worse_label   \n",
       "25  cifar-10n                ensemble (all)  worse_label   \n",
       "26  cifar-10n                ensemble (all)  worse_label   \n",
       "27  cifar-10n                ensemble (all)  worse_label   \n",
       "28  cifar-10n                ensemble (all)  worse_label   \n",
       "29  cifar-10n                ensemble (all)  worse_label   \n",
       "\n",
       "                         method  adjust_pred_probs     auroc  lift_at_1000  \\\n",
       "0               self_confidence              False  0.963435      2.487067   \n",
       "1               self_confidence               True  0.956146      2.484580   \n",
       "2             normalized_margin              False  0.966803      2.487067   \n",
       "3             normalized_margin               True  0.966067      2.484580   \n",
       "4   confidence_weighted_entropy              False  0.954042      2.487067   \n",
       "5               self_confidence              False  0.975780      2.487067   \n",
       "6               self_confidence               True  0.971084      2.484580   \n",
       "7             normalized_margin              False  0.979797      2.482093   \n",
       "8             normalized_margin               True  0.980028      2.484580   \n",
       "9   confidence_weighted_entropy              False  0.968417      2.487067   \n",
       "10              self_confidence              False  0.964984      2.487067   \n",
       "11              self_confidence               True  0.957359      2.477119   \n",
       "12            normalized_margin              False  0.970835      2.484580   \n",
       "13            normalized_margin               True  0.970507      2.479606   \n",
       "14  confidence_weighted_entropy              False  0.955172      2.484580   \n",
       "15              self_confidence              False  0.984722      2.487067   \n",
       "16              self_confidence               True  0.981895      2.487067   \n",
       "17            normalized_margin              False  0.986910      2.487067   \n",
       "18            normalized_margin               True  0.987353      2.484580   \n",
       "19  confidence_weighted_entropy              False  0.980211      2.487067   \n",
       "20              self_confidence              False  0.990451      2.487067   \n",
       "21              self_confidence               True  0.989189      2.487067   \n",
       "22            normalized_margin              False  0.992099      2.487067   \n",
       "23            normalized_margin               True  0.992943      2.487067   \n",
       "24  confidence_weighted_entropy              False  0.986798      2.487067   \n",
       "25              self_confidence              False  0.989748      2.487067   \n",
       "26              self_confidence               True  0.987316      2.487067   \n",
       "27            normalized_margin              False  0.992681      2.487067   \n",
       "28            normalized_margin               True  0.993323      2.487067   \n",
       "29  confidence_weighted_entropy              False  0.984303      2.487067   \n",
       "\n",
       "    lift_at_2000  lift_at_3000  lift_at_4000  lift_at_5000  lift_at_6000  \\\n",
       "0       2.487067      2.487067      2.485202      2.484580      2.482508   \n",
       "1       2.470901      2.453077      2.439813      2.431357      2.433595   \n",
       "2       2.482093      2.482093      2.477119      2.472642      2.468414   \n",
       "3       2.479606      2.476290      2.473388      2.469658      2.465098   \n",
       "4       2.484580      2.482093      2.480850      2.480601      2.477948   \n",
       "5       2.487067      2.485409      2.485824      2.485575      2.483751   \n",
       "6       2.458466      2.458051      2.460331      2.460207      2.458880   \n",
       "7       2.480850      2.479606      2.477741      2.475129      2.472559   \n",
       "8       2.479606      2.477119      2.477119      2.473637      2.471730   \n",
       "9       2.485824      2.486238      2.483958      2.482591      2.480850   \n",
       "10      2.487067      2.483751      2.483337      2.482093      2.478363   \n",
       "11      2.458466      2.447274      2.441678      2.437823      2.429450   \n",
       "12      2.484580      2.482922      2.480850      2.478611      2.473388   \n",
       "13      2.483337      2.480435      2.476497      2.474135      2.470901   \n",
       "14      2.482093      2.481264      2.479606      2.477119      2.474217   \n",
       "15      2.487067      2.486238      2.485824      2.486072      2.484580   \n",
       "16      2.472145      2.476290      2.475254      2.474632      2.475046   \n",
       "17      2.484580      2.482093      2.482715      2.482093      2.481679   \n",
       "18      2.483337      2.482922      2.482715      2.483088      2.480435   \n",
       "19      2.485824      2.486238      2.485202      2.485078      2.484995   \n",
       "20      2.487067      2.487067      2.487067      2.487067      2.487067   \n",
       "21      2.485824      2.485409      2.483958      2.483585      2.482922   \n",
       "22      2.487067      2.486238      2.485202      2.485078      2.485409   \n",
       "23      2.487067      2.487067      2.487067      2.485575      2.484166   \n",
       "24      2.487067      2.487067      2.487067      2.486072      2.486238   \n",
       "25      2.487067      2.487067      2.487067      2.487067      2.487067   \n",
       "26      2.484580      2.485409      2.484580      2.484580      2.484166   \n",
       "27      2.487067      2.487067      2.487067      2.487067      2.487067   \n",
       "28      2.487067      2.487067      2.487067      2.486570      2.486238   \n",
       "29      2.487067      2.487067      2.487067      2.486570      2.486238   \n",
       "\n",
       "    lift_at_7000  lift_at_8000  lift_at_9000  lift_at_10000  lift_at_11000  \\\n",
       "0       2.479606      2.474321      2.469381       2.460953       2.449987   \n",
       "1       2.433062      2.427067      2.422127       2.417927       2.406577   \n",
       "2       2.462552      2.457533      2.452525       2.446777       2.435291   \n",
       "3       2.461486      2.456290      2.451972       2.445036       2.433708   \n",
       "4       2.473566      2.466549      2.457499       2.444787       2.430091   \n",
       "5       2.482448      2.480539      2.477119       2.472394       2.468075   \n",
       "6       2.453669      2.451937      2.451419       2.448766       2.444335   \n",
       "7       2.469658      2.468414      2.466342       2.464684       2.460840   \n",
       "8       2.471079      2.468103      2.464407       2.462197       2.457901   \n",
       "9       2.477830      2.474321      2.471592       2.466176       2.458805   \n",
       "10      2.477474      2.474321      2.471039       2.463689       2.448179   \n",
       "11      2.424180      2.421160      2.416048       2.411460       2.405898   \n",
       "12      2.465750      2.458777      2.453630       2.448518       2.441396   \n",
       "13      2.466105      2.460020      2.453630       2.447025       2.439361   \n",
       "14      2.471790      2.468103      2.460262       2.447025       2.434839   \n",
       "15      2.483870      2.483958      2.482922       2.481596       2.479606   \n",
       "16      2.472500      2.468103      2.467171       2.466673       2.464458   \n",
       "17      2.480672      2.479295      2.477672       2.474881       2.473049   \n",
       "18      2.479606      2.477741      2.477395       2.475875       2.473501   \n",
       "19      2.483870      2.482093      2.480988       2.479109       2.476441   \n",
       "20      2.486712      2.486135      2.485962       2.485326       2.484806   \n",
       "21      2.481027      2.479917      2.480711       2.480103       2.479832   \n",
       "22      2.484580      2.484269      2.484304       2.483834       2.482998   \n",
       "23      2.484225      2.483958      2.484304       2.483088       2.482545   \n",
       "24      2.485291      2.485202      2.484028       2.483585       2.482771   \n",
       "25      2.486357      2.486445      2.486238       2.485575       2.484806   \n",
       "26      2.483159      2.483337      2.482646       2.481098       2.479380   \n",
       "27      2.486001      2.486135      2.486238       2.485824       2.484354   \n",
       "28      2.486001      2.485824      2.485962       2.486072       2.485711   \n",
       "29      2.486357      2.485824      2.484304       2.484083       2.482545   \n",
       "\n",
       "    lift_at_12000  lift_at_13000  lift_at_14000  lift_at_15000  lift_at_16000  \\\n",
       "0        2.435668       2.418386       2.396645       2.372165       2.338465   \n",
       "1        2.395046       2.377636       2.357917       2.336185       2.306755   \n",
       "2        2.423440       2.411499       2.396289       2.379129       2.357895   \n",
       "3        2.421574       2.409203       2.393802       2.372994       2.347791   \n",
       "4        2.411833       2.390454       2.360049       2.331377       2.294630   \n",
       "5        2.459088       2.445361       2.426845       2.404828       2.379035   \n",
       "6        2.438569       2.425273       2.409258       2.386921       2.360227   \n",
       "7        2.456393       2.448613       2.441057       2.427212       2.413543   \n",
       "8        2.455772       2.451674       2.439991       2.427875       2.413232   \n",
       "9        2.448310       2.433691       2.411745       2.377968       2.345149   \n",
       "10       2.434424       2.414177       2.388650       2.359729       2.333957   \n",
       "11       2.393180       2.378210       2.357029       2.333366       2.300537   \n",
       "12       2.433388       2.421064       2.410679       2.393222       2.372973   \n",
       "13       2.430901       2.419151       2.407836       2.386756       2.368465   \n",
       "14       2.413906       2.391411       2.357740       2.327729       2.291832   \n",
       "15       2.477326       2.471380       2.461841       2.448103       2.431108   \n",
       "16       2.463233       2.457222       2.445675       2.431854       2.414165   \n",
       "17       2.470487       2.466405       2.462374       2.456725       2.449917   \n",
       "18       2.471316       2.468127       2.463262       2.457388       2.447274   \n",
       "19       2.472145       2.465640       2.454202       2.433844       2.405616   \n",
       "20       2.483129       2.481328       2.479606       2.472476       2.463285   \n",
       "21       2.478570       2.476736       2.473211       2.468497       2.458155   \n",
       "22       2.480228       2.479032       2.478007       2.474300       2.471057   \n",
       "23       2.481471       2.479797       2.478363       2.476290       2.472145   \n",
       "24       2.482508       2.479797       2.473744       2.464849       2.449139   \n",
       "25       2.483129       2.480945       2.475165       2.467337       2.452559   \n",
       "26       2.476912       2.473675       2.465927       2.455067       2.438725   \n",
       "27       2.483544       2.482476       2.480139       2.478777       2.474943   \n",
       "28       2.484373       2.483050       2.481027       2.478114       2.474787   \n",
       "29       2.480021       2.474632       2.464861       2.447606       2.421315   \n",
       "\n",
       "    lift_at_17000  lift_at_18000  lift_at_19000  lift_at_20000  \n",
       "0        2.307852       2.267929       2.226972       2.178547  \n",
       "1        2.275228       2.238913       2.200138       2.153552  \n",
       "2        2.328041       2.293767       2.252628       2.202795  \n",
       "3        2.319995       2.286582       2.247654       2.201925  \n",
       "4        2.257964       2.220675       2.174613       2.127064  \n",
       "5        2.354960       2.325546       2.291767       2.249925  \n",
       "6        2.333454       2.304820       2.270823       2.233759  \n",
       "7        2.396362       2.366168       2.330382       2.284620  \n",
       "8        2.394314       2.365063       2.329989       2.284869  \n",
       "9        2.310924       2.278154       2.242549       2.204909  \n",
       "10       2.302439       2.267376       2.226449       2.186256  \n",
       "11       2.268498       2.237117       2.198567       2.158401  \n",
       "12       2.347645       2.317256       2.280117       2.235127  \n",
       "13       2.343988       2.311176       2.273834       2.228412  \n",
       "14       2.254307       2.216253       2.174875       2.133779  \n",
       "15       2.411285       2.388275       2.358394       2.318444  \n",
       "16       2.397825       2.373077       2.342556       2.301159  \n",
       "17       2.437033       2.419226       2.386537       2.338092  \n",
       "18       2.434839       2.416462       2.382349       2.336848  \n",
       "19       2.377929       2.351246       2.321219       2.278154  \n",
       "20       2.453419       2.437188       2.412193       2.363709  \n",
       "21       2.445080       2.429588       2.402245       2.356372  \n",
       "22       2.465123       2.452110       2.423189       2.376890  \n",
       "23       2.466147       2.454874       2.428687       2.382983  \n",
       "24       2.432644       2.409830       2.382087       2.335107  \n",
       "25       2.437911       2.417153       2.389417       2.344931  \n",
       "26       2.422111       2.399191       2.367950       2.322796  \n",
       "27       2.464537       2.448241       2.420178       2.374527  \n",
       "28       2.464976       2.449623       2.423058       2.375771  \n",
       "29       2.396948       2.368655       2.335356       2.292579  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d3a755-7185-4d36-b392-c517c1607707",
   "metadata": {},
   "source": [
    "## Export results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a41642c3-ce1e-44a3-a596-87dbe135c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results to CSV file\n",
    "ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "df.to_csv(f\"label_quality_scores_evaluation_{ts}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab20eb-fc5c-438b-96e0-eeec6f11d88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16460097-f9a3-4ce4-a520-441dbd15616c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea67e42-000b-4a68-8913-9173ecc5b417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4571a328-b632-4ea6-814f-396677adb959",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa1da22a-cb5b-4363-9e78-bc273b76eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"swin_base_patch4_window7_224\"\n",
    "# model = \"twins_pcpvt_base\"\n",
    "\n",
    "# read numpy files\n",
    "numpy_out_folder = f\"./cifar-10n-png_noise_type_worst_cv_{model}/\"\n",
    "\n",
    "pred_probs = np.load(numpy_out_folder + \"pred_probs.npy\")\n",
    "labels = np.load(numpy_out_folder + \"noisy_labels.npy\")\n",
    "true_labels = np.load(numpy_out_folder + \"true_labels.npy\")\n",
    "images = np.load(numpy_out_folder + \"images.npy\", allow_pickle=True)\n",
    "\n",
    "# boolean mask of label errors\n",
    "label_errors_target = labels != true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "feebb2a5-d8c7-4fc3-8df6-244279e48df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40208"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_errors_target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cc2881f-ec4e-4e79-a242-0b3945a82fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_issues = cleanlab.filter.find_label_issues(labels=labels, pred_probs=pred_probs, filter_by=\"predicted_neq_given\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1e49dd7-e08d-42e8-b01d-a4d15cba11e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19704"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_issues.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa6cc4fb-1569-47a2-b576-94afe3d112f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19704"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datapoints where pred_prob argmax does not equal the noisy label\n",
    "(pred_probs.argmax(axis=1) != labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aef9731f-ffa5-41dd-980f-d5c66765dd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94302"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_probs.argmax(axis=1) == true_labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22582a51-ef89-4951-9a64-2e5ff042b9a0",
   "metadata": {},
   "source": [
    "## Evaluate different filter_by options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc12eedb-4b35-4f06-871c-2ab4a112a910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filter_by': 'prune_by_noise_rate', 'num_label_issues': 17872, 'f1_score': 0.9236359806193385, 'precision': 0.9813115487914056, 'recall': 0.8723637087146836}\n",
      "{'filter_by': 'prune_by_class', 'num_label_issues': 18347, 'f1_score': 0.9383891186185015, 'precision': 0.98332152395487, 'recall': 0.897383605252686}\n",
      "{'filter_by': 'both', 'num_label_issues': 17280, 'f1_score': 0.9133319066980528, 'precision': 0.9879629629629629, 'recall': 0.8491842419419021}\n",
      "{'filter_by': 'confident_learning', 'num_label_issues': 16888, 'f1_score': 0.8917063148788927, 'precision': 0.976610611084794, 'recall': 0.8203840031834461}\n",
      "{'filter_by': 'predicted_neq_given', 'num_label_issues': 19704, 'f1_score': 0.9527230707395499, 'precision': 0.9623934226552984, 'recall': 0.9432451253481894}\n"
     ]
    }
   ],
   "source": [
    "filter_by_list = [\n",
    "    \"prune_by_noise_rate\",\n",
    "    \"prune_by_class\",\n",
    "    \"both\",\n",
    "    \"confident_learning\",\n",
    "    \"predicted_neq_given\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for filter_by in filter_by_list:\n",
    "\n",
    "    label_issues = find_label_issues(\n",
    "        labels=labels,\n",
    "        pred_probs=pred_probs,\n",
    "        filter_by=filter_by,\n",
    "    )\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    f1 = f1_score(label_errors_target, label_issues)\n",
    "    precision = precision_score(label_errors_target, label_issues)\n",
    "    recall = recall_score(label_errors_target, label_issues)\n",
    "\n",
    "    result = {\n",
    "        \"filter_by\": filter_by,\n",
    "        \"num_label_issues\": sum(label_issues),\n",
    "        \"f1_score\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "    print(result)\n",
    "    \n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45e0d3c0-1e3a-4167-a0b6-11be9c0e840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: swin_base_patch4_window7_224\n",
      "cross-val procedure: stratified k-folds (k=5)\n",
      "dataset: cifar-10n\n",
      "noise type: worse (40% noise rate)\n",
      "\n",
      "Label Error Detection: Evaluating Different filter_by:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filter_by</th>\n",
       "      <th>num_label_issues</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prune_by_noise_rate</td>\n",
       "      <td>17872</td>\n",
       "      <td>0.923636</td>\n",
       "      <td>0.981312</td>\n",
       "      <td>0.872364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prune_by_class</td>\n",
       "      <td>18347</td>\n",
       "      <td>0.938389</td>\n",
       "      <td>0.983322</td>\n",
       "      <td>0.897384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>both</td>\n",
       "      <td>17280</td>\n",
       "      <td>0.913332</td>\n",
       "      <td>0.987963</td>\n",
       "      <td>0.849184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>confident_learning</td>\n",
       "      <td>16888</td>\n",
       "      <td>0.891706</td>\n",
       "      <td>0.976611</td>\n",
       "      <td>0.820384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>predicted_neq_given</td>\n",
       "      <td>19704</td>\n",
       "      <td>0.952723</td>\n",
       "      <td>0.962393</td>\n",
       "      <td>0.943245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             filter_by  num_label_issues  f1_score  precision    recall\n",
       "0  prune_by_noise_rate             17872  0.923636   0.981312  0.872364\n",
       "1       prune_by_class             18347  0.938389   0.983322  0.897384\n",
       "2                 both             17280  0.913332   0.987963  0.849184\n",
       "3   confident_learning             16888  0.891706   0.976611  0.820384\n",
       "4  predicted_neq_given             19704  0.952723   0.962393  0.943245"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"model: {model}\")\n",
    "print(f\"cross-val procedure: stratified k-folds (k=5)\")\n",
    "print(f\"dataset: cifar-10n\")\n",
    "print(f\"noise type: worse (40% noise rate)\")\n",
    "print()\n",
    "print(f\"Label Error Detection: Evaluating Different filter_by:\")\n",
    "print()\n",
    "pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
