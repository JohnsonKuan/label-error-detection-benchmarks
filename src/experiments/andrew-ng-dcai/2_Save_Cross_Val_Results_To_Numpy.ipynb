{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae8e07-99a8-48b4-bf35-0c3b7b5a02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from label_errors import get_label_errors\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from eval_metrics import lift_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b110d03-632d-42eb-8299-df29bd4062a6",
   "metadata": {},
   "source": [
    "## Get list of label errors (ground truth from manual review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b14b3-ce15-4ed5-a2d7-4cba8f26b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATION_PATH = \"andrew-ng-dcai-comp-2021-manual-review-for-label-errors.xlsx\"\n",
    "label_errors = get_label_errors(annotation_path=ANNOTATION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc83d1-3682-4f22-a911-ba959aa96675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out some files with label error\n",
    "label_errors[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93dba5e-83a6-43c9-81b0-d430d31b869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of label errors: {len(label_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb9bc1-544a-418e-8f17-31c3c091e1f7",
   "metadata": {},
   "source": [
    "## Save to numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cc52a-0c01-4122-a246-3b42b9ec6546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions below (can consider moving these to separate utils.py file)\n",
    "\n",
    "def load_pickle(pickle_file_name, verbose=1):\n",
    "    \"\"\"Load pickle file\"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Loading {pickle_file_name}\")\n",
    "\n",
    "    with open(pickle_file_name, 'rb') as handle:\n",
    "        out = pickle.load(handle)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b896e-c7e1-41c9-b522-9dfe44424d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cv_folds = 5 # number K in stratified K-folds cross-validation\n",
    "verbose = 0\n",
    "\n",
    "models = [\n",
    "    \"resnet18\", \n",
    "    \"resnet50d\",\n",
    "    \"efficientnet_b1\",\n",
    "    \"twins_pcpvt_base\",\n",
    "    \"swin_base_patch4_window7_224\"\n",
    "]\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    pred_probs = []\n",
    "    labels = []\n",
    "    images = []\n",
    "    \n",
    "    for split_num in range(num_cv_folds):\n",
    "\n",
    "        out_subfolder = f\"./dcai_train_val_dataset_cv_{model}_20220329175851/split_{split_num}/\"\n",
    "        \n",
    "        # pickle file name to read\n",
    "        get_pickle_file_name = (\n",
    "            lambda object_name: f\"{out_subfolder}_{object_name}_split_{split_num}\"\n",
    "        )\n",
    "\n",
    "        # NOTE: the \"test_\" prefix in the pickle name correspond to the \"test\" split during cross-validation.\n",
    "        pred_probs_split = load_pickle(get_pickle_file_name(\"test_pred_probs\"), verbose=verbose)\n",
    "        labels_split = load_pickle(get_pickle_file_name(\"test_labels\"), verbose=verbose)\n",
    "        images_split = load_pickle(get_pickle_file_name(\"test_image_files\"), verbose=verbose)\n",
    "        indices_split = load_pickle(get_pickle_file_name(\"test_indices\"), verbose=verbose)\n",
    "\n",
    "        # append to list so we can combine data from all the splits\n",
    "        pred_probs.append(pred_probs_split)\n",
    "        labels.append(labels_split)\n",
    "        images.append(images_split)    \n",
    "\n",
    "    # convert list to array\n",
    "    pred_probs = np.vstack(pred_probs)\n",
    "    labels = np.hstack(labels)\n",
    "    images = np.hstack(images)\n",
    "    \n",
    "    # label error binary target\n",
    "    label_errors_mask = pd.Series(images).map(lambda x: Path(x).name in label_errors).values    \n",
    "    \n",
    "    # save to Numpy files\n",
    "    numpy_out_folder = f\"./dcai_train_val_dataset_cv_{model}/\"\n",
    "    \n",
    "    print(f\"Saving to numpy files in this folder: {numpy_out_folder}\")\n",
    "    \n",
    "    np.save(numpy_out_folder + \"pred_probs\", pred_probs)\n",
    "    np.save(numpy_out_folder + \"labels\", labels)\n",
    "    np.save(numpy_out_folder + \"images\", images)\n",
    "    np.save(numpy_out_folder + \"label_errors_mask\", label_errors_mask)\n",
    "\n",
    "    # check the accuracy\n",
    "    acc_labels = (pred_probs.argmax(axis=1) == labels).mean()\n",
    "    \n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"  Accuracy (argmax pred vs noisy labels): {acc_labels}\")\n",
    "    \n",
    "    results = {\n",
    "        \"model\": model,\n",
    "        \"Accuracy (argmax pred vs noisy labels)\": acc_labels,\n",
    "    }\n",
    "    \n",
    "    results_list.append(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
